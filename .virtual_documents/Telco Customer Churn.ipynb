











# Code



































# Basic
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
import warnings

warnings.filterwarnings("ignore")
# pd.set_option("display.max_columns", None)
pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", 500)
pd.set_option("display.max_colwidth", 200)

# Test
from scipy.stats import normaltest, chi2_contingency, ttest_ind
from scipy import stats

# Preprocessing
from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer
from sklearn.impute import SimpleImputer
import category_encoders as ce
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import RobustScaler
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectPercentile
from sklearn.svm import SVC

# Imbalance Dataset
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE, RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler, NearMiss

# Modeling
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import (
    RandomForestClassifier,
    AdaBoostClassifier,
    GradientBoostingClassifier,
)
import xgboost as xgb
from xgboost import XGBClassifier
import lightgbm as lgb

# Evaluation
from sklearn.model_selection import (
    cross_validate,
    GridSearchCV,
    StratifiedKFold,
    train_test_split,
    cross_val_score,
    cross_val_predict,
)
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import (
    make_scorer,
    fbeta_score,
    recall_score,
    precision_score,
    balanced_accuracy_score,
)
from sklearn.metrics import roc_auc_score, RocCurveDisplay
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import VotingClassifier, RandomForestClassifier


import pickle











base = pd.read_csv("dataset/WA_Fn-UseC_-Telco-Customer-Churn.csv")
base





status = pd.read_csv("dataset/Tambahan/Telco_customer_churn_status.csv", sep=";")
status = status[["Customer ID", "Satisfaction Score", "CLTV"]]
status





demographic = pd.read_csv(
    "dataset/Tambahan/Telco_customer_churn_demographics.csv", sep=";"
)
demographic = demographic[["Customer ID", "Under 30"]]
demographic





# Join all table
df = base.merge(status, left_on="customerID", right_on="Customer ID", how="left").merge(
    demographic, left_on="customerID", right_on="Customer ID", how="left"
)

df = df[
    [
        "customerID",
        "gender",
        "Under 30",
        "SeniorCitizen",
        "Partner",
        "PhoneService",
        "MultipleLines",
        "InternetService",
        "OnlineSecurity",
        "OnlineBackup",
        "DeviceProtection",
        "TechSupport",
        "StreamingTV",
        "StreamingMovies",
        "Contract",
        "PaperlessBilling",
        "PaymentMethod",
        "tenure",
        "MonthlyCharges",
        "TotalCharges",
        "CLTV",
        "Satisfaction Score",
        "Churn",
    ]
]

df.sample(5)


























df_check = pd.DataFrame(
    {
        "Features": df.columns,
        "Data Type": df.dtypes.values,
        "Data Count": df.count().values,
        "Null Value Count": df.isnull().sum().values,
        "Number of Unique Value": df.nunique().values,
        "Min Value": [
            df[col].min() if pd.api.types.is_numeric_dtype(df[col]) else ""
            for col in df.columns
        ],
        "Max Value": [
            df[col].max() if pd.api.types.is_numeric_dtype(df[col]) else ""
            for col in df.columns
        ],
        "Unique Value": [df[col].unique() for col in df.columns],
    }
)

display(df_check)





df["SeniorCitizen"] = df["SeniorCitizen"].map({0: "No", 1: "Yes"})





df[pd.to_numeric(df["TotalCharges"], errors="coerce").isna()]





df = df[df["TotalCharges"] != " "]





df[["TotalCharges"]] = df[["TotalCharges"]].astype(float)





df["Satisfaction Score"] = pd.Categorical(
    df["Satisfaction Score"], categories=[1, 2, 3, 4, 5], ordered=True
)











outlier_counts = {}
for col in df.select_dtypes(include=[np.number]).columns:
    q1 = df[col].quantile(0.25)
    q3 = df[col].quantile(0.75)
    iqr = q3 - q1
    lower = q1 - 1.5 * iqr
    upper = q3 + 1.5 * iqr
    outliers = df[(df[col] < lower) | (df[col] > upper)].shape[0]
    outlier_counts[col] = outliers

outlier_df = pd.DataFrame.from_dict(
    outlier_counts, orient="index", columns=["Outlier Count"]
)

outlier_df.sort_values(by="Outlier Count", ascending=False)








pd.DataFrame(df.duplicated().value_counts())








df["AgeGroup"] = pd.NA
df.loc[df["Under 30"] == "Yes", "AgeGroup"] = "Under 30"
df.loc[(df["Under 30"] == "No") & (df["SeniorCitizen"] == "No"), "AgeGroup"] = (
    "Middle Age (30–59)"
)
df.loc[df["SeniorCitizen"] == "Yes", "AgeGroup"] = "Senior (≥ 60)"

# Set AgeGroup as categorical with order
df["AgeGroup"] = pd.Categorical(
    df["AgeGroup"],
    categories=["Under 30", "Middle Age (30–59)", "Senior (≥ 60)"],
    ordered=True,
)














def showQuantiDist(col):
    fig, axes = plt.subplots(1, 3, figsize=(18, 4))

    # Histogram with Set2 palette (manually apply color)
    sns.histplot(
        df[col].dropna(), ax=axes[0], kde=True, color=sns.color_palette("Set2")[0]
    )
    axes[0].set_title(f"Histogram of {col}")

    # Boxplot with Set2 palette
    sns.boxplot(x=df[col], ax=axes[1], color=sns.color_palette("Set2")[1])
    axes[1].set_title(f"Boxplot of {col}")

    # QQPlot (color not directly applicable, but set manually)
    stats.probplot(df[col].dropna(), dist="norm", plot=axes[2])
    axes[2].get_lines()[0].set_color(sns.color_palette("Set2")[2])  # Data points
    axes[2].get_lines()[1].set_color("black")  # QQ line
    axes[2].set_title(f"QQ Plot of {col}")

    fig.tight_layout()
    plt.show()

    # Inferential Analysis
    churned = df[df["Churn"] == "Yes"][col]
    not_churned = df[df["Churn"] == "No"][col]
    t_stat, p = ttest_ind(churned, not_churned, equal_var=False)

    ttest_result = {
        "Feature": col,
        "t-statistic": round(t_stat, 2),
        "p-value": round(p, 4),
        "Significant (p < 0.05)": p < 0.05,
    }
    display(pd.DataFrame([ttest_result]))





showQuantiDist("tenure")








showQuantiDist("MonthlyCharges")








showQuantiDist("TotalCharges")








showQuantiDist("CLTV")








numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()

# Compute spearman correlation matrix
pearson_corr = df[numerical_features].corr(method="spearman")

# Plot the correlation heatmap
plt.figure(figsize=(12, 6))
sns.heatmap(pearson_corr, annot=True, cmap="BuGn", fmt=".2f", square=True)
plt.title("Spearman Correlation Matrix (Numerical Features)")
plt.tight_layout()
plt.show()








def showQualiDist(col):
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))

    # Bar chart
    sns.countplot(
        data=df, x=col, order=df[col].value_counts().index, ax=axes[0], palette="Set2"
    )
    axes[0].set_title(f"Bar Chart of {col}")
    axes[0].set_ylabel("Count")
    axes[0].tick_params(axis="x", rotation=45)

    # Annotate counts
    for p in axes[0].patches:
        height = p.get_height()
        axes[0].annotate(
            f"{round(height/1000, 1)}K",
            (p.get_x() + p.get_width() / 2.0, height - 200),
            ha="center",
            va="bottom",
            fontsize=10,
        )

    # Pie chart
    pie_data = df[col].value_counts()
    axes[1].pie(
        pie_data,
        labels=pie_data.index,
        autopct="%1.1f%%",
        startangle=90,
        colors=sns.color_palette("Set2"),
    )
    axes[1].set_title(f"Pie Chart of {col}")
    axes[1].axis("equal")  # Equal aspect ratio ensures pie is a circle

    plt.tight_layout()
    plt.show()

    # Chi-Square Test
    chi2_table = pd.crosstab(df[col], df["Churn"])
    chi2, p, dof, ex = chi2_contingency(chi2_table)

    chi_result = {
        "Feature": col,
        "Chi-square": round(chi2, 2),
        "p-value": round(p, 4),
        "Significant (p < 0.05)": p < 0.05,
    }
    display(pd.DataFrame([chi_result]))





showQualiDist("gender")








showQualiDist("Partner")








showQualiDist("AgeGroup")








showQualiDist("PhoneService")








showQualiDist("MultipleLines")








showQualiDist("InternetService")








showQualiDist("OnlineSecurity")








showQualiDist("OnlineBackup")








showQualiDist("DeviceProtection")








showQualiDist("TechSupport")








showQualiDist("StreamingTV")








showQualiDist("StreamingMovies")








showQualiDist("Contract")








showQualiDist("PaperlessBilling")








showQualiDist("PaymentMethod")








showQualiDist("Churn")
# "YES" di highlight di explode








showQualiDist("Satisfaction Score")








df = df[
    [
        "customerID",
        "Partner",
        "MultipleLines",
        "InternetService",
        "OnlineSecurity",
        "OnlineBackup",
        "DeviceProtection",
        "TechSupport",
        "StreamingTV",
        "StreamingMovies",
        "Contract",
        "PaperlessBilling",
        "PaymentMethod",
        "tenure",
        "MonthlyCharges",
        "CLTV",
        "Satisfaction Score",
        "Churn",
        "AgeGroup",
    ]
]


df.to_csv("Dataset/df_clean.csv", index=False)














def showQuantiChurn(col, unit=""):
    fig, axes = plt.subplots(figsize=(8, 4))
    sns.boxplot(data=df, y="Churn", x=col, palette="Set2")
    plt.title(f"{col.capitalize()} by Churn")
    if unit != "":
        plt.xlabel(f"{col.capitalize()} ({unit})")
    else:
        plt.xlabel(col.capitalize())
    plt.ylabel("Churn")
    plt.tight_layout()
    plt.show()





showQuantiChurn("tenure", "Month")








showQuantiChurn("MonthlyCharges", "$")








showQuantiChurn("CLTV", "$")








def showQualiChurn(col, unit=""):
    fig, axes = plt.subplots(1, 2, figsize=(16, 5))

    # Vertical Bar Chart
    churn_counts = df.groupby([col, "Churn"]).size().unstack(fill_value=0)
    churn_counts.plot(
        kind="bar", stacked=False, ax=axes[0], color=sns.color_palette("Set2")
    )

    for i, category in enumerate(churn_counts.index):
        for j, churn_value in enumerate(["No", "Yes"]):
            if churn_value in churn_counts.columns:
                count = churn_counts.loc[category, churn_value]
                axes[0].text(
                    i + (j - 0.5) * 0.2,
                    count,
                    f"{round(count/1000, 1)}K",
                    ha="center",
                    va="bottom",
                )

    axes[0].set_title(f"Churn Count by {col}")
    axes[0].set_xlabel(col)
    axes[0].set_ylabel("Count")
    axes[0].legend(title="Churn")

    # Horizontal Bar Chart
    churn_rate = (
        churn_counts["Yes"] / (churn_counts["Yes"] + churn_counts["No"])
    ) * 100
    churn_rate = churn_rate.sort_values(ascending=False)

    churn_counts.index = churn_counts.index.astype(str)
    churn_rate.index = churn_rate.index.astype(str)

    sns.barplot(x=churn_rate.values, y=churn_rate.index, ax=axes[1], color="coral")
    for i, (val, name) in enumerate(zip(churn_rate.values, churn_rate.index)):
        axes[1].text(val + 0.5, i, f"{val:.1f}%", va="center")

    overall_churn = df["Churn"].value_counts(normalize=True)["Yes"] * 100
    axes[1].axvline(
        overall_churn,
        color="red",
        linestyle="--",
        linewidth=1.5,
        label=f"Overall Churn: {overall_churn:.1f}%",
    )
    axes[1].legend(loc="lower right")
    axes[1].set_title(f"Churn Rate per Category (Yes / Total in Category)")
    axes[1].set_xlabel("Churn Rate (%)")
    axes[1].set_ylabel(col)

    plt.tight_layout()
    plt.show()





showQualiChurn("Partner")








showQualiChurn("AgeGroup")








showQualiChurn("MultipleLines")








showQualiChurn("InternetService")








showQualiChurn("OnlineSecurity")








showQualiChurn("OnlineBackup")








showQualiChurn("DeviceProtection")








showQualiChurn("TechSupport")








showQualiChurn("StreamingTV")








showQualiChurn("StreamingMovies")








showQualiChurn("Contract")








showQualiChurn("PaperlessBilling")








showQualiChurn("PaymentMethod")








showQualiChurn("Satisfaction Score")











df_analytic = df[["OnlineSecurity", "TechSupport", "DeviceProtection", "Churn"]].copy()





df_analytic["RuleBased_Churn_Prediction"] = (
    (df_analytic["OnlineSecurity"] == "No")
    | (df_analytic["TechSupport"] == "No")
    | (df_analytic["DeviceProtection"] == "No")
).astype(int)

rule_based_churn_summary = (
    df_analytic["RuleBased_Churn_Prediction"]
    .value_counts()
    .rename(index={0: "Not Churned", 1: "Churned"})
    .reset_index()
)
rule_based_churn_summary.columns = ["Churn Prediction", "Customer Count"]
rule_based_churn_summary














df["Churn"] = df["Churn"].map({"No": 0, "Yes": 1})








X = df.drop(columns=["customerID", "Churn"])
y = df["Churn"]








X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)








num_cols = X_train.select_dtypes(include=["int64", "float64"]).columns.tolist()
cat_cols = X_train.select_dtypes(include=["object", "category"]).columns.tolist()

num_transformer = Pipeline(
    [("imputer", SimpleImputer(strategy="median")), ("scaler", RobustScaler())]
)

cat_transformer = Pipeline(
    [
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(drop="first", handle_unknown="ignore")),
    ]
)

preprocessor = ColumnTransformer(
    [("num", num_transformer, num_cols), ("cat", cat_transformer, cat_cols)],
    remainder="passthrough",
)
preprocessor








models = {
    "Logistic Regression": LogisticRegression(),
    "KNN": KNeighborsClassifier(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "XGBoost": XGBClassifier(eval_metric="logloss", use_label_encoder=False),
    "LightGBM": lgb.LGBMClassifier(),
    "SVM": SVC(probability=True),
}








# Setup CV strategy
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

f2_scores = []
roc_aucs = []

# Cross-validated model evaluation
for name, model in models.items():
    pipe = Pipeline([("preprocess", preprocessor), ("model", model)])

    # Get cross-validated predictions (probabilities and labels)
    y_pred = cross_val_predict(pipe, X_train, y_train, cv=cv, method="predict")
    y_proba = cross_val_predict(pipe, X_train, y_train, cv=cv, method="predict_proba")[
        :, 1
    ]

    # Calculate evaluation metrics
    f2 = fbeta_score(y_train, y_pred, beta=2)
    roc = roc_auc_score(y_train, y_proba)

    f2_scores.append(f2)
    roc_aucs.append(roc)

# Compile results into a DataFrame
results_df = (
    pd.DataFrame(
        {
            "Model": list(models.keys()),
            "Mean F2 Score": sum(f2_scores) / len(f2_scores),
            "Mean ROC AUC": sum(roc_aucs) / len(roc_aucs),
        }
    )
    .set_index("Model")
    .sort_values(by="Mean F2 Score", ascending=False)
)

print("\nModel Benchmarking Results:")
display(results_df)








# Logistic Regression pipeline
logreg_pipe = Pipeline(
    [("preprocess", preprocessor), ("model", LogisticRegression(max_iter=1000))]
)

# Hyperparameter grid
param_grid_logreg = {
    "model__penalty": ["l1", "l2"],
    "model__C": [0.01, 0.1, 1, 10],
    "model__solver": ["liblinear", "saga"],
}

grid_search = GridSearchCV(
    estimator=logreg_pipe,
    param_grid=param_grid_logreg,
    scoring="f1",
    cv=5,
    verbose=1,
    n_jobs=-1,
)

grid_search.fit(X_train, y_train)
best_model = grid_search.best_estimator_

# Evaluate
y_pred = best_model.predict(X_test)
y_proba = best_model.predict_proba(X_test)[:, 1]

f2 = fbeta_score(y_test, y_pred, beta=2)
roc_auc = roc_auc_score(y_test, y_proba)

# Show results
results = pd.DataFrame(
    {"Model": ["Tuned Logistic Regression"], "F2 Score": [f2], "ROC AUC": [roc_auc]}
).set_index("Model")

display(results)








logreg_model = best_model.named_steps["model"]

# Extract encoded categorical feature names
ohe = (
    best_model.named_steps["preprocess"]
    .named_transformers_["cat"]
    .named_steps["onehot"]
)
cat_feature_names = ohe.get_feature_names_out(cat_cols)

all_feature_names = num_cols + cat_feature_names.tolist()

coefficients = logreg_model.coef_[0]

feature_importance_df = pd.DataFrame(
    {
        "Feature": all_feature_names,
        "Coefficient": coefficients,
        "Abs_Coefficient": np.abs(coefficients),
    }
).sort_values(by="Abs_Coefficient", ascending=False)

# Assuming you already have `feature_importance_df` from your trained model
top_features = feature_importance_df.sort_values(
    by="Abs_Coefficient", ascending=False
).head(20)

plt.figure(figsize=(10, 8))
bars = plt.barh(top_features["Feature"], top_features["Coefficient"], color="skyblue")
plt.axvline(0, color="black", linewidth=0.8)
plt.xlabel("Coefficient Value")
plt.title(f"Top 5 Feature Importances - Logistic Regression")
plt.gca().invert_yaxis()  # Puts most important at the top

plt.tight_layout()
plt.show()








model_filename = "tuned_logistic_regression.pkl"

with open(model_filename, "wb") as file:
    pickle.dump(best_model, file)

print(f"Model saved to {model_filename}")











df["Churn"].value_counts()


1869 * 0.878049












